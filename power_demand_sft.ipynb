{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전력수요 예측 특화 LLM - SFT (Supervised Fine-Tuning)\n",
    "\n",
    "## 접근법\n",
    "1. **GPT API**로 전력수요 문서 → Q&A 쌍 생성\n",
    "2. **SFT (지도학습)**으로 Instruction-tuned 모델 파인튜닝\n",
    "3. 자기지도학습(DAPT) ❌, 지도학습(SFT) ✅\n",
    "\n",
    "## 왜 SFT?\n",
    "- Instruct 모델의 질문-답변 능력 유지\n",
    "- 전력수요 도메인 지식 학습\n",
    "- DAPT는 instruction 능력을 깨뜨릴 위험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# !pip install transformers datasets torch accelerate bitsandbytes peft trl sentencepiece openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/De-Qwen-SFT/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/De-Qwen-SFT/.venv/lib/python3.13/site-packages/trl/__init__.py:203: UserWarning: TRL currently supports vLLM versions: 0.10.2, 0.11.0, 0.11.1, 0.11.2. You have version 0.13.0 installed. We recommend installing a supported version to avoid compatibility issues.\n",
      "  if is_vllm_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-80GB\n",
      "CUDA memory: 85.09 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q&A 데이터셋 생성 (GPT API 사용)\n",
    "\n",
    "먼저 `generate_qa_dataset.py` 스크립트를 실행하여 Q&A 데이터셋을 생성 -> 생성완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 생성된 Q&A 데이터셋 로딩 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 총 1139개 Q&A 쌍 로딩 완료\n",
      "\n",
      "=== 샘플 데이터 ===\n",
      "\n",
      "[1] 질문: 2019년 12월의 최대부하는 얼마인가요?\n",
      "    답변: 2019년 12월의 최대부하는 8,730만kW로 예측되었습니다....\n",
      "\n",
      "[2] 질문: What was the maximum load in December 2018?\n",
      "    답변: The maximum load in December 2018 was 8,622 만kW....\n",
      "\n",
      "[3] 질문: 2019년 12월의 주별 최대 전력 수요는 어떻게 되나요?\n",
      "    답변: 2019년 12월의 주별 최대 전력 수요는 1주차 8,240만 kW, 2주차 8,420만 kW, 3주차 8,540만 kW, 4주차 8,730만 kW로 증가하는 추세를 보였습니다....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SFT 데이터셋 로딩\n",
    "dataset_file = \"sft_dataset.jsonl\"\n",
    "\n",
    "if not Path(dataset_file).exists():\n",
    "    print(f\"❌ {dataset_file} 파일이 없습니다.\")\n",
    "    print(\"먼저 generate_qa_dataset.py를 실행하여 데이터셋을 생성하세요.\")\n",
    "else:\n",
    "    # JSONL 파일 로딩\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        sft_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"✓ 총 {len(sft_data)}개 Q&A 쌍 로딩 완료\")\n",
    "    \n",
    "    # 샘플 확인\n",
    "    print(\"\\n=== 샘플 데이터 ===\\n\")\n",
    "    for i, sample in enumerate(sft_data[:3], 1):\n",
    "        print(f\"[{i}] 질문: {sample['messages'][0]['content']}\")\n",
    "        print(f\"    답변: {sample['messages'][1]['content'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환 시작: /root/De-Qwen-SFT/power_demand_sft_full.jsonl -> /root/De-Qwen-SFT/power_demand_sft_full_converted.jsonl\n",
      "변환 완료! 총 585개의 데이터가 변환되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = '/root/De-Qwen-SFT/power_demand_sft_full.jsonl'\n",
    "output_file = '/root/De-Qwen-SFT/power_demand_sft_full_converted.jsonl'\n",
    "\n",
    "print(f\"변환 시작: {input_file} -> {output_file}\")\n",
    "\n",
    "count = 0\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # User Content 구성: Instruction + Input (Input이 있는 경우)\n",
    "        user_content = data.get('instruction', '')\n",
    "        input_text = data.get('input', '')\n",
    "        if input_text:\n",
    "            user_content += f\"\\n\\n{input_text}\"\n",
    "            \n",
    "        # Assistant Content 구성: Output\n",
    "        assistant_content = data.get('output', '')\n",
    "        \n",
    "        # 새로운 포맷 (Messages)으로 변환\n",
    "        new_data = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        outfile.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "        count += 1\n",
    "\n",
    "print(f\"변환 완료! 총 {count}개의 데이터가 변환되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1724 examples [00:00, 188694.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1551개\n",
      "Test set: 173개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 수정 전\n",
    "# dataset_file = \"/root/De-Qwen-SFT/sft_dataset.jsonl\"\n",
    "# dataset = load_dataset('json', data_files=dataset_file, split='train')\n",
    "\n",
    "# 수정 후: 두 파일 경로를 리스트로 전달\n",
    "dataset_files = [\n",
    "    \"/root/De-Qwen-SFT/sft_dataset.jsonl\",\n",
    "    \"/root/De-Qwen-SFT/power_demand_sft_full_converted.jsonl\"  # 변환한 파일\n",
    "]\n",
    "\n",
    "# 자동으로 두 파일을 합쳐서 로드합니다\n",
    "dataset = load_dataset('json', data_files=dataset_files, split='train')\n",
    "\n",
    "# 이후 코드는 그대로 사용 (합쳐진 전체 데이터에서 10%를 테스트 셋으로 분리)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train set: {len(dataset['train'])}개\")\n",
    "print(f\"Test set: {len(dataset['test'])}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 및 토크나이저 로딩\n",
    "\n",
    "### 추천 모델:\n",
    "1. **EEVE-Korean-10.8B**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`\n",
    "2. **Qwen2.5-7B**: `Qwen/Qwen2.5-7B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: /root/models/llama-3-korean-bllossom-8B\n"
     ]
    }
   ],
   "source": [
    "# 모델 선택\n",
    "model_name = \"/root/models/llama-3-korean-bllossom-8B\"  # Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "print(f\"Loading model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded\n",
      "  Vocab size: 128256\n",
      "  Pad token: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# padding token 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /root/models/llama-3-korean-bllossom-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded successfully\n",
      "  Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 1. 경로 수정 (방금 다운로드 받은 폴더 이름으로 변경)\n",
    "model_path = \"/root/models/llama-3-korean-bllossom-8B\"\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "# 2. 4bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 3. 토크나이저 로드 (이게 있어야 글자를 이해합니다)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 4. 모델 로딩\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"  Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습 전 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "=== Chat Template ===\n",
      "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "\n",
      "=== Special Tokens ===\n",
      "EOS token: <|eot_id|> -> 128009\n",
      "BOS token: <|begin_of_text|> -> 128000\n",
      "PAD token: <|end_of_text|> -> 128001\n",
      "\n",
      "=== Special Token IDs ===\n",
      "<|begin_of_text|> -> 128000\n",
      "<|start_header_id|> -> 128006\n",
      "<|end_header_id|> -> 128007\n",
      "<|eot_id|> -> 128009\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)\n",
    "\n",
    "# 실제 chat template 확인\n",
    "print(\"=== Chat Template ===\")\n",
    "print(tokenizer.chat_template)\n",
    "\n",
    "print(\"\\n=== Special Tokens ===\")\n",
    "print(f\"EOS token: {tokenizer.eos_token} -> {tokenizer.eos_token_id}\")\n",
    "print(f\"BOS token: {tokenizer.bos_token} -> {tokenizer.bos_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} -> {tokenizer.pad_token_id}\")\n",
    "\n",
    "# 특수 토큰들이 실제로 존재하는지 확인\n",
    "print(\"\\n=== Special Token IDs ===\")\n",
    "for token in [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"{token} -> {token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "학습 전 모델 응답 테스트\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 2019년 2월 전력수요 최대부하는 얼마인가요?\n",
      "답변: 2019년 2월의 전력 수요는 특정한 날짜에 대한 정보가므로, 정확한 값을 제공하기 위해서는 해당 시기의 전력 수요 데이터를 확인해야 합니다. 그러나, 전력 수요는 날씨, 경제 활동, 인구 밀집 등 다양한 요인에 의해 변동되기 때문에, 특정한 날짜의 전력 수요를 예측하기 어렵습니다.\n",
      "\n",
      "그러나, 전 세계적으로 전력 수요는 일반적으로 연중과 계절에 따라 변동합니다. 예를 들어, 겨울철에는 난방이 필요할 수 있지만, 여름철에는 공급량이 줄어야 할 수 있습니다. 또한, 특정한 지역이나\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 전력수요 예측에서 기상 요인의 영향을 설명해주세요.\n",
      "답변: 기상 요인(Weather Factor)들이 전력 수요 예측에 미치는 영향은 다음과 같습니다.\n",
      "\n",
      "1. **기온**: 기온이 높거나 낮을 때, 전력 소비가 달라집니다.\n",
      "   - 높은 기온: 에어컨 사용 증가로 전력 소비 증가\n",
      "   - 낮은 기온: 난방 장비 사용 증가로 전력 소비 증가\n",
      "\n",
      "2. **습도**: 습도가 높을 때, 냉난방 장비의 전력 소비가 증가합니다.\n",
      "   - 습도 증가: 공기 조화 및 냉난방 장비 사용 증가로 전력 소비 증가\n",
      "\n",
      "3. **풍속**: 강한 바람\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What is the peak electricity demand in summer?\n",
      "답변: The peak electricity demand, also known as peak load or peak power demand, varies by region and country. It typically occurs during hot summer afternoons when air conditioning usage is highest. Here are some examples of peak electricity demands in different regions:\n",
      "\n",
      "1. **United States**: The peak electricity demand in the United States usually occurs on hot summer days (July-August) between 4 pm and 6 pm local time. According to the U.S. Energy Information Administration (EIA), the peak demand was around 650 gigawatts (GW) in 2020.\n",
      "2. **Europe**: In Europe, peak electricity demand often occurs during heatwaves, which can occur at any time of year. For example:\n",
      "\t* **Germany**:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def format_llama3_chat(prompt: str) -> str:\n",
    "    # Llama-3는 Header 뒤에 줄바꿈이 2개(\\n\\n) 들어가야 합니다.\n",
    "    return (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"  # 수정: \\n -> \\n\\n\n",
    "        f\"{prompt}<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"  # 수정: \\n -> \\n\\n\n",
    "    )\n",
    "\n",
    "def test_model(prompt, max_new_tokens=200):\n",
    "    text = format_llama3_chat(prompt)\n",
    "    \n",
    "    # tokenizer와 model이 전역 변수로 있다고 가정\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # pad_token_id 설정 (Llama 3는 보통 end_of_text 토큰을 pad로 씀)\n",
    "    # config에 따라 다르지만 보통 128001(<|end_of_text|>) 사용\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 128001\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=pad_token_id, \n",
    "            eos_token_id=[128001, 128009],  # <|end_of_text|>, <|eot_id|> 모두 종료로 인식\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "\n",
    "    # 입력 토큰 이후부터 디코딩\n",
    "    generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# --- 테스트 실행 ---\n",
    "test_questions = [\n",
    "    \"2019년 2월 전력수요 최대부하는 얼마인가요?\",\n",
    "    \"전력수요 예측에서 기상 요인의 영향을 설명해주세요.\",\n",
    "    \"What is the peak electricity demand in summer?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"학습 전 모델 응답 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=150)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 포맷팅 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 포맷팅 전 ===\n",
      "[{'role': 'user', 'content': 'What are the expected precipitation levels for June 2021?'}, {'role': 'assistant', 'content': 'The expected precipitation levels for June 2021 are similar or more than average, with a probability of 40% each.'}]\n",
      "\n",
      "=== 포맷팅 후 ===\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the expected precipitation levels for June 2021?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The expected precipitation levels for June 2021 are similar or more than average, with a probability of 40% each.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],   # ❗ 그대로 전달\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "# 샘플 확인\n",
    "sample = dataset['train'][0]\n",
    "print(\"=== 포맷팅 전 ===\")\n",
    "print(sample['messages'])\n",
    "print(\"\\n=== 포맷팅 후 ===\")\n",
    "print(formatting_func({'messages': [sample['messages']]})[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SFT 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 학습 파라미터\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./power_demand_sft_output_llama3\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # GPU 메모리에 맞게 조정\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # ▼▼▼ 수정된 부분 (evaluation_strategy -> eval_strategy) ▼▼▼\n",
    "    eval_strategy=\"steps\", \n",
    "    # ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # wandb 사용 시 \"wandb\"로 변경\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A6000에 최적화된 모델 함수\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 1. SFTConfig\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./power_demand_sft_output_llama3\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✓ SFT Config configured\")\n",
    "\n",
    "# 2. SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,   # ✅ tokenizer는 여기\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SFT Config configured (A100 Optimized)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 1551/1551 [00:00<00:00, 7392.52 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1551/1551 [00:00<00:00, 1912.13 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1551/1551 [00:00<00:00, 237856.14 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 173/173 [00:00<00:00, 6450.14 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 173/173 [00:00<00:00, 1826.37 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 173/173 [00:00<00:00, 48565.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## A100 최적화\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 1. SFTConfig (A100 80GB 최적화)\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./power_demand_sft_output_llama3\",\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # A100의 대용량 메모리 활용\n",
    "    per_device_train_batch_size=16,  # 2 -> 16 (메모리 여유에 따라 32까지도 가능)\n",
    "    per_device_eval_batch_size=16,   # 2 -> 16\n",
    "    \n",
    "    # 배치 사이즈가 커졌으므로 accumulation은 줄여도 됨 (Total Batch = 16 * 1 * 1 = 16)\n",
    "    gradient_accumulation_steps=1,   # 4 -> 1\n",
    "    \n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,      # A100은 bf16 지원 (필수)\n",
    "    \n",
    "    # 로깅 및 저장 주기 조정\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=10,\n",
    "    \n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # 추가 최적화 옵션\n",
    "    gradient_checkpointing=True,     # 메모리 절약 (큰 배치 사이즈 가능하게 함)\n",
    "    dataloader_num_workers=4,        # 데이터 로딩 속도 향상\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✓ SFT Config configured (A100 Optimized)\")\n",
    "\n",
    "# 2. SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,   # ✅ tokenizer는 여기\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dtype=torch.float16)\n",
    "model.config.torch_dtype = torch.float16\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SFT 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128001}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n",
      "Total training samples: 1551\n",
      "Total eval samples: 173\n",
      "Effective batch size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 16:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.582300</td>\n",
       "      <td>0.552288</td>\n",
       "      <td>0.607198</td>\n",
       "      <td>136071.000000</td>\n",
       "      <td>0.850849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.391773</td>\n",
       "      <td>0.427721</td>\n",
       "      <td>260711.000000</td>\n",
       "      <td>0.882777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.358923</td>\n",
       "      <td>0.369956</td>\n",
       "      <td>395761.000000</td>\n",
       "      <td>0.888805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.336040</td>\n",
       "      <td>0.338133</td>\n",
       "      <td>518657.000000</td>\n",
       "      <td>0.893952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.322758</td>\n",
       "      <td>0.326286</td>\n",
       "      <td>653650.000000</td>\n",
       "      <td>0.898179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting SFT training...\")\n",
    "print(f\"Total training samples: {len(dataset['train'])}\")\n",
    "print(f\"Total eval samples: {len(dataset['test'])}\")\n",
    "\n",
    "effective_batch_size = (\n",
    "    sft_config.per_device_train_batch_size\n",
    "    * sft_config.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to ./power_demand_sft_model_llama3\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 저장\n",
    "output_dir = \"./power_demand_sft_model_llama3\"\n",
    "\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✓ Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습 후 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "학습 후 모델 응답\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 2019년 2월 전력수요 최대부하는 얼마인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변: 2019년 2월 전력수요 최대부하는 8,760만 kW로 예측되었습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 전력수요 예측에서 기상 요인의 영향을 설명해주세요.\n",
      "답변: 2024년 1월 전력 수요는 평년보다 높을 것으로 예상되며, 기온과 강수량이 평년보다 낮고 건조할 확률이 50%입니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What is the peak electricity demand in summer?\n",
      "답변: The peak electricity demand in summer is projected to be 93,400 MW.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"학습 후 모델 응답\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=150)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "추가 테스트\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 여름철 전력수요가 높은 이유는?\n",
      "답변: 2019년 8월과 9월은 기온이 높고 강수량이 많을 것으로 예상되며, 이는 전력 수요를 증가시킬 수 있습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 2023년 최대 전력수요는 언제 발생했나요?\n",
      "답변: 2023년 최대 전력수요는 8월에 92,700㎿로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What are the main factors affecting power demand?\n",
      "답변: The main factors affecting power demand include temperature, precipitation, and seasonal changes.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 질문: 2025년 1월의 평균 전력수요는 어떻게 될 것으로 생각하나요?\n",
      "답변: 2025년 1월의 평균 전력수요는 65,800㎿로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 질문: 평년 대비 전력수요 증가율을 어떻게 계산하나요?\n",
      "답변: 평년 대비 전력수요 증가율은 전력수요의 평균치와 비교하여 계산되며, 이는 수요가 평년보다 얼마나 증가했는지 나타냅니다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 추가 테스트 질문\n",
    "additional_questions = [\n",
    "    \"여름철 전력수요가 높은 이유는?\",\n",
    "    \"2023년 최대 전력수요는 언제 발생했나요?\",\n",
    "    \"What are the main factors affecting power demand?\",\n",
    "    \"2025년 1월의 평균 전력수요는 어떻게 될 것으로 생각하나요?\",\n",
    "    \"평년 대비 전력수요 증가율을 어떻게 계산하나요?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"추가 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(additional_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=200)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test Set 샘플 평가\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: What was the trend in maximum load from 2017 to 2021?\n",
      "\n",
      "정답: The maximum load showed an increase in 2018, then a decrease in 2019 and 2020, followed by an increase again in 2021.\n",
      "\n",
      "예측: The maximum load showed a general increasing trend from 2017 to 2020 but decreased slightly in 2021.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 2022년 6월의 평균 전력수요는 어떻게 되나요?\n",
      "\n",
      "정답: 2022년 6월의 평균 전력수요는 61,700㎿로 예측되고 있습니다.\n",
      "\n",
      "예측: 2022년 6월의 평균 전력수요는 61,700㎿로 예측됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: Describe the weekly electricity demand pattern for September 2020.\n",
      "\n",
      "이 표는 2020년 9월의 주별 기상 예측을 나타내고 있습니다. 각 주의 기온과 강수량에 대한 예측이 포함되어 있습니다.\n",
      "\n",
      "1주 (8.31~9.6):\n",
      "- 덥고 습한 공기의 영향을 받다가 상층 찬 공기의 영향을 일시적으로 받아 기온 변화가 크고, 발달한 저기압과 대기 불안정으로 많은 비가 내릴 때가 있겠습니다.\n",
      "- 평년 대비 기온과 강수량은 비슷하겠습니다.\n",
      "\n",
      "2주 (9.7~9.13):\n",
      "- 상층 찬 공기의 영향을 받겠으나, 낮 동안에는 일사로 인해 더운 날이 있겠습니다.\n",
      "- 평년 대비 기온은 비슷하고, 강수량은 비슷하거나 많겠습니다.\n",
      "\n",
      "3주 (9.14~9.20):\n",
      "- 건조한 공기(이동성 고기압)의 영향을 주로 받겠으며, 낮 동안에는 일사로 인해 더운 날이 많겠습니다.\n",
      "- 평년 대비 기온은 비슷하거나 높겠으며, 강수량은 비슷하겠습니다.\n",
      "\n",
      "4주 (9.21~9.27):\n",
      "- 건조한 공기(이동성 고기압)의 영향을 주로 받겠으나, 남쪽을 지나는 발달한 저기압의 영향으로 많은 비가 내릴 때가 있겠습니다.\n",
      "- 평년 대비 기온은 높겠고, 강수량은 비슷하겠습니다.\n",
      "\n",
      "이 표는 주별로 기온과 강수량의 변화를 예측하여, 기상 조건에 따른 생활 계획에 참고할 수 있도록 돕습니다.\n",
      "\n",
      "정답: The weekly demand data show typical intra-month variations.\n",
      "\n",
      "예측: The weekly demand data show typical intra-month variations.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 질문: 2023년 9월의 평균 전력 수요는 얼마인가요?\n",
      "\n",
      "정답: 2023년 9월의 평균 전력 수요는 61,600 MW로 예측됩니다.\n",
      "\n",
      "예측: 2023년 9월의 평균 전력 수요는 68,700㎿로 예측됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 질문: 2021년 2월의 기온과 강수량 예측은 어떤가요?\n",
      "\n",
      "정답: 2월은 대륙고기압과 이동성 고기압의 영향을 받아 기온 변화가 크고, 강수량은 평년과 비슷하겠으며, 대체로 맑고 건조한 날이 많을 것으로 예상됩니다.\n",
      "\n",
      "예측: 2021년 2월 기온은 평년보다 높을 확률이 50%이며, 강수량은 평년과 비슷할 확률이 40%, 적을 확률이 10%입니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] 질문: Describe the weekly electricity demand pattern for June 2024.\n",
      "\n",
      "The table presents the projected maximum electricity demand (in megawatts, MW) for each week of July 2024. Here's a detailed summary:\n",
      "\n",
      "- **1st Week (7/1 - 7/7):** The maximum demand is projected to be 86,000 MW.\n",
      "- **2nd Week (7/8 - 7/14):** The demand increases to 89,400 MW.\n",
      "- **3rd Week (7/15 - 7/21):** The demand further rises to 91,000 MW.\n",
      "- **4th Week (7/22 - 7/28):** The highest demand is expected, reaching 93,400 MW.\n",
      "\n",
      "**Trend Analysis:**\n",
      "- There is a consistent upward trend in electricity demand throughout the month.\n",
      "- The demand increases by 3,400 MW from the 1st to the 2nd week, 1,600 MW from the 2nd to the 3rd week, and 2,400 MW from the 3rd to the 4th week.\n",
      "- Overall, the demand grows by 7,400 MW from the beginning to the end of the month.\n",
      "\n",
      "정답: For June 2024, weekly maximum demand ranges from 86,000 MW in Week 1 to 93,400 MW in Week 4, showing a 8.6% increase.\n",
      "\n",
      "예측: For June 2024, weekly maximum demand ranges from 86,000 MW in Week 1 to 93,400 MW in Week 4, showing a 8.6% increase.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test set 샘플로 성능 평가\n",
    "test_samples = dataset['test'].select(range(min(10, len(dataset['test']))))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test Set 샘플 평가\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    question = sample['messages'][0]['content']\n",
    "    expected = sample['messages'][1]['content']\n",
    "    predicted = test_model(question, max_new_tokens=200)\n",
    "    \n",
    "    print(f\"\\n[{i}] 질문: {question}\")\n",
    "    print(f\"\\n정답: {expected}\")\n",
    "    print(f\"\\n예측: {predicted}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 결론\n",
    "\n",
    "### SFT 접근법의 장점:\n",
    "- ✅ Instruction-following 능력 유지\n",
    "- ✅ 전력수요 도메인 지식 학습\n",
    "- ✅ 질문-답변 형태로 즉시 사용 가능\n",
    "\n",
    "### 다음 단계:\n",
    "1. **더 많은 데이터**: GPT API로 전체 125개 파일 처리\n",
    "2. **하이퍼파라미터 튜닝**: learning rate, epochs 조정\n",
    "3. **평가 메트릭**: BLEU, ROUGE 등으로 정량 평가\n",
    "4. **배포**: FastAPI 등으로 서빙\n",
    "\n",
    "### 비용 고려:\n",
    "- GPT-4o-mini: 약 $0.15/1M input tokens\n",
    "- 125개 파일 × 평균 5000 tokens = 625K tokens\n",
    "- 예상 비용: ~$0.10 (매우 저렴)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
