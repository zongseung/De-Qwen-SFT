{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전력수요 예측 특화 LLM - SFT (Supervised Fine-Tuning)\n",
    "\n",
    "## 접근법\n",
    "1. **GPT API**로 전력수요 문서 → Q&A 쌍 생성\n",
    "2. **SFT (지도학습)**으로 Instruction-tuned 모델 파인튜닝\n",
    "3. 자기지도학습(DAPT) ❌, 지도학습(SFT) ✅\n",
    "\n",
    "## 왜 SFT?\n",
    "- Instruct 모델의 질문-답변 능력 유지\n",
    "- 전력수요 도메인 지식 학습\n",
    "- DAPT는 instruction 능력을 깨뜨릴 위험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "# !pip install transformers datasets torch accelerate bitsandbytes peft trl sentencepiece openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/de-llama/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX A6000\n",
      "CUDA memory: 51.04 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q&A 데이터셋 생성 (GPT API 사용)\n",
    "\n",
    "먼저 `generate_qa_dataset.py` 스크립트를 실행하여 Q&A 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OPENAI_API_KEY 설정됨\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API 키 설정 확인\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  OPENAI_API_KEY가 설정되지 않았습니다.\")\n",
    "    print(\"\\n다음 명령어로 설정하세요:\")\n",
    "    print(\"export OPENAI_API_KEY='your-api-key-here'\")\n",
    "else:\n",
    "    print(\"✓ OPENAI_API_KEY 설정됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A 데이터셋 생성 스크립트 실행\n",
    "# 터미널에서 실행:\n",
    "# python generate_qa_dataset.py\n",
    "\n",
    "# 또는 여기서 직접 실행 (테스트용, 5개 파일만)\n",
    "# %run generate_qa_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 생성된 Q&A 데이터셋 로딩 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 총 1139개 Q&A 쌍 로딩 완료\n",
      "\n",
      "=== 샘플 데이터 ===\n",
      "\n",
      "[1] 질문: 2019년 12월의 최대부하는 얼마인가요?\n",
      "    답변: 2019년 12월의 최대부하는 8,730만kW로 예측되었습니다....\n",
      "\n",
      "[2] 질문: What was the maximum load in December 2018?\n",
      "    답변: The maximum load in December 2018 was 8,622 만kW....\n",
      "\n",
      "[3] 질문: 2019년 12월의 주별 최대 전력 수요는 어떻게 되나요?\n",
      "    답변: 2019년 12월의 주별 최대 전력 수요는 1주차 8,240만 kW, 2주차 8,420만 kW, 3주차 8,540만 kW, 4주차 8,730만 kW로 증가하는 추세를 보였습니다....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SFT 데이터셋 로딩\n",
    "dataset_file = \"sft_dataset.jsonl\"\n",
    "\n",
    "if not Path(dataset_file).exists():\n",
    "    print(f\"❌ {dataset_file} 파일이 없습니다.\")\n",
    "    print(\"먼저 generate_qa_dataset.py를 실행하여 데이터셋을 생성하세요.\")\n",
    "else:\n",
    "    # JSONL 파일 로딩\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        sft_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"✓ 총 {len(sft_data)}개 Q&A 쌍 로딩 완료\")\n",
    "    \n",
    "    # 샘플 확인\n",
    "    print(\"\\n=== 샘플 데이터 ===\\n\")\n",
    "    for i, sample in enumerate(sft_data[:3], 1):\n",
    "        print(f\"[{i}] 질문: {sample['messages'][0]['content']}\")\n",
    "        print(f\"    답변: {sample['messages'][1]['content'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1025개\n",
      "Test set: 114개\n"
     ]
    }
   ],
   "source": [
    "# Dataset 객체로 변환\n",
    "dataset = load_dataset('json', data_files=dataset_file, split='train')\n",
    "\n",
    "# Train/Test 분할\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train set: {len(dataset['train'])}개\")\n",
    "print(f\"Test set: {len(dataset['test'])}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 및 토크나이저 로딩\n",
    "\n",
    "### 추천 모델:\n",
    "1. **EEVE-Korean-10.8B**: `yanolja/EEVE-Korean-Instruct-10.8B-v1.0`\n",
    "2. **Qwen2.5-7B**: `Qwen/Qwen2.5-7B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 모델 선택\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"  # 또는 \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer loaded\n",
      "  Vocab size: 151665\n",
      "  Pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로딩\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# padding token 설정\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"✓ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "  Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 모델 로딩\n",
    "model_name = \"./model_weights\"  # <-- 여기가 핵심입니다!\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True, # 인터넷 연결 안 하고 로컬 파일만 쓰겠다는 강력한 옵션\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)   # torch.float16 이어야 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습 전 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "학습 전 모델 응답\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 2019년 2월 전력수요 최대부하는 얼마인가요?\n",
      "답변: 죄송합니다만, 저는 실시간 데이터 접근이 불가능하므로 2019년 2월의 전력수요 최대부하에 대한 정확한 정보를 제공하기는 어렵습니다. 이 정보는 지역 전력 회사 또는 에너지 통계 기관에서 제공할 수 있습니다. 일반적으로 이런 정보는 각국의 에너지 부처 웹사이트나 전력 공급사에서 확인 가능합니다. 필요한 경우 특정 국가의 전력 공급사 웹사이트를 방문하여 관련 정보를 확인해 보시기 바랍니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 전력수요 예측에서 기상 요인의 영향을 설명해주세요.\n",
      "답변: 전력 수요 예측에서 기상 요인은 중요한 역할을 합니다. 기상 조건은 사람들의 활동 패턴과 에너지 사용에 직접적인 영향을 미칩니다. 다음은 기상 요인의 주요 영향 요소들입니다:\n",
      "\n",
      "1. **기온**: 고온 또는 저온은 전력 수요를 크게 변화시킵니다. 일반적으로 낮은 온도에서는 가정이나 건물 내에서 더 많은 에어컨이 작동하여 전력 사용량이 증가합니다. 반대로 높은 온도에서는 냉방 시설이 작동하여 전력을 많이 소비하게 됩니다.\n",
      "\n",
      "2.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What is the peak electricity demand in summer?\n",
      "답변: Peak electricity demand in summer is often attributed to higher usage of air conditioning systems due to hot temperatures. In many regions, this peak demand can occur during the afternoon when people return home from work and turn on air conditioning units, fans, and other cooling devices.\n",
      "\n",
      "The exact peak demand varies significantly depending on the location, climate, and time of day. For example:\n",
      "\n",
      "1. **Location**: Areas with hotter climates or those that experience rapid temperature changes will have higher peak demands compared to cooler regions.\n",
      "2. **Time of Day**: Peak demand typically occurs between 3 PM and 7 PM in many places, but this can vary.\n",
      "3. **Seasonal Factors**: The summer season brings higher temperatures, increasing the need for cooling, which drives up\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_model(prompt, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    모델 테스트 함수\n",
    "    \"\"\"\n",
    "    # Chat template 적용\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # 입력 부분 제거하고 생성된 부분만 추출\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "# 테스트 질문\n",
    "test_questions = [\n",
    "    \"2019년 2월 전력수요 최대부하는 얼마인가요?\",\n",
    "    \"전력수요 예측에서 기상 요인의 영향을 설명해주세요.\",\n",
    "    \"What is the peak electricity demand in summer?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"학습 전 모델 응답\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=150)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,092,544 || all params: 7,625,709,056 || trainable%: 0.1323\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 데이터 포맷팅 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatting_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(ex))                 \u001b[38;5;66;03m# dict\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]))     \u001b[38;5;66;03m# list\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mformatting_func\u001b[49m(ex))      \u001b[38;5;66;03m# str\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatting_func' is not defined"
     ]
    }
   ],
   "source": [
    "ex = dataset[\"train\"][0]\n",
    "print(type(ex))                 # dict\n",
    "print(type(ex[\"messages\"]))     # list\n",
    "print(formatting_func(ex))      # str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 포맷팅 전 ===\n",
      "[{'role': 'user', 'content': 'What is the probability of temperatures being higher than normal in June 2022?'}, {'role': 'assistant', 'content': 'There is a 40% probability of temperatures being higher than normal in June 2022.'}]\n",
      "\n",
      "=== 포맷팅 후 ===\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the probability of temperatures being higher than normal in June 2022?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "There is a 40% probability of temperatures being higher than normal in Jun\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],   # ❗ 그대로 전달\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "# 샘플 확인\n",
    "sample = dataset['train'][0]\n",
    "print(\"=== 포맷팅 전 ===\")\n",
    "print(sample['messages'])\n",
    "print(\"\\n=== 포맷팅 후 ===\")\n",
    "print(formatting_func({'messages': [sample['messages']]})[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SFT 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 학습 파라미터\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./power_demand_sft_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # GPU 메모리에 맞게 조정\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # ▼▼▼ 수정된 부분 (evaluation_strategy -> eval_strategy) ▼▼▼\n",
    "    eval_strategy=\"steps\", \n",
    "    # ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # wandb 사용 시 \"wandb\"로 변경\n",
    ")\n",
    "\n",
    "print(\"✓ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "print(trl.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SFT Config configured\n",
      "✓ Trainer created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 1. SFTConfig\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./power_demand_sft_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✓ SFT Config configured\")\n",
    "\n",
    "# 2. SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,   # ✅ tokenizer는 여기\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(dtype=torch.float16)\n",
    "model.config.torch_dtype = torch.float16\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SFT 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n",
      "Total training samples: 1025\n",
      "Total eval samples: 114\n",
      "Effective batch size: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='387' max='387' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [387/387 11:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>0.488842</td>\n",
       "      <td>0.478886</td>\n",
       "      <td>34105.000000</td>\n",
       "      <td>0.850611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.440628</td>\n",
       "      <td>0.445243</td>\n",
       "      <td>68475.000000</td>\n",
       "      <td>0.858440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.422178</td>\n",
       "      <td>0.399563</td>\n",
       "      <td>102443.000000</td>\n",
       "      <td>0.866042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.363200</td>\n",
       "      <td>0.408742</td>\n",
       "      <td>0.388732</td>\n",
       "      <td>136613.000000</td>\n",
       "      <td>0.869839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.399079</td>\n",
       "      <td>0.377015</td>\n",
       "      <td>170883.000000</td>\n",
       "      <td>0.873807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.397866</td>\n",
       "      <td>0.351923</td>\n",
       "      <td>204477.000000</td>\n",
       "      <td>0.873293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.393987</td>\n",
       "      <td>0.355785</td>\n",
       "      <td>239506.000000</td>\n",
       "      <td>0.872360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting SFT training...\")\n",
    "print(f\"Total training samples: {len(dataset['train'])}\")\n",
    "print(f\"Total eval samples: {len(dataset['test'])}\")\n",
    "\n",
    "effective_batch_size = (\n",
    "    sft_config.per_device_train_batch_size\n",
    "    * sft_config.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to ./power_demand_sft_model\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 저장\n",
    "output_dir = \"./power_demand_sft_model\"\n",
    "\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"✓ Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 학습 후 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "학습 후 모델 응답\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 2019년 2월 전력수요 최대부하는 얼마인가요?\n",
      "답변: 2019년 2월 전력수요 최대부하는 8,650만 kW로 예측되었습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 전력수요 예측에서 기상 요인의 영향을 설명해주세요.\n",
      "답변: 기상 요인은 전력 수요에 큰 영향을 줄 수 있습니다. 예를 들어, 날씨가 춥거나 덥고 습할 경우, 가정과 비즈니스에서 더 많은 에너지를 사용하게 됩니다. 이는 주로 공기 조건과 같은 기상 요인에 의해 발생합니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What is the peak electricity demand in summer?\n",
      "답변: The peak electricity demand in summer (June to August) is projected to be 90,400 MW.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 학습 후 테스트\n",
    "print(\"=\" * 80)\n",
    "print(\"학습 후 모델 응답\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=150)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "추가 테스트\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: 여름철 전력수요가 높은 이유는?\n",
      "답변: 여름철 전력수요는 날씨가 더운데도 불구하고 기온이 상대적으로 낮게 예측되거나, 대량의 폭염이 예상될 때 높을 가능성이 있습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 2023년 최대 전력수요는 언제 발생했나요?\n",
      "답변: 2023년 최대 전력수요는 9월에 85,700㎿로 발생했습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: What are the main factors affecting power demand?\n",
      "답변: The main factors affecting power demand include weather conditions, economic growth, and public awareness of energy efficiency.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 질문: 2025년 1월의 평균 전력수요는 어떻게 될 것으로 생각하나요?\n",
      "답변: 2025년 1월의 평균 전력수요는 67,800㎿로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 질문: 평년 대비 전력수요 증가율을 어떻게 계산하나요?\n",
      "답변: 전력수요 증가율은 평년 대비 전력 수요의 변화를 나타내며, 2024년 1월 평년 대비 증가율은 6.8%로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 추가 테스트 질문\n",
    "additional_questions = [\n",
    "    \"여름철 전력수요가 높은 이유는?\",\n",
    "    \"2023년 최대 전력수요는 언제 발생했나요?\",\n",
    "    \"What are the main factors affecting power demand?\",\n",
    "    \"2025년 1월의 평균 전력수요는 어떻게 될 것으로 생각하나요?\",\n",
    "    \"평년 대비 전력수요 증가율을 어떻게 계산하나요?\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"추가 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(additional_questions, 1):\n",
    "    print(f\"\\n[{i}] 질문: {q}\")\n",
    "    print(f\"답변: {test_model(q, max_new_tokens=200)}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Test Set 샘플 평가\n",
      "================================================================================\n",
      "\n",
      "[1] 질문: What is the forecast for weekly maximum electricity demand in October 2024?\n",
      "\n",
      "정답: The weekly maximum electricity demand in October 2024 is forecasted as follows: Week 1: 77,000 MW, Week 2: 73,400 MW, Week 3: 73,000 MW, Week 4: 72,700 MW, Week 5: 74,900 MW.\n",
      "\n",
      "예측: The forecast for weekly maximum electricity demand in October 2024 ranges from 76,100 MW to 81,700 MW across four weeks.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] 질문: 전력 수요는 보통 어떤 계절에 가장 높나요?\n",
      "\n",
      "정답: 전력 수요는 일반적으로 겨울과 여름에 가장 높고, 봄과 가을에는 낮은 경향이 있습니다.\n",
      "\n",
      "예측: 전력 수요는 여름(7-8월)에 가장 높으며, 겨울(12-2월)에 차순으로 높습니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] 질문: 2024년 10월의 전력 수요 전망 결과는 어떻게 되나요?\n",
      "\n",
      "정답: 2024년 10월의 전력 수요 전망 결과는 60,200㎿로, 여러 모델의 평균을 반영한 수치입니다.\n",
      "\n",
      "예측: 2024년 10월의 전력 수요는 69,500 MW로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] 질문: What was the highest electricity demand recorded in August 2023?\n",
      "\n",
      "정답: The highest electricity demand recorded in August 2023 was 94,509 MW.\n",
      "\n",
      "예측: The highest electricity demand recorded in August 2023 was 95.9GW on August 16th.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] 질문: What is the expected trend for average temperatures in January 2021?\n",
      "\n",
      "정답: The average temperatures are expected to be low initially but increase to similar or high probabilities as the month progresses.\n",
      "\n",
      "예측: Average temperatures are expected to be similar to or higher than the normal range of -3.9°C to 0.7°C throughout January 2021.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[6] 질문: 2022년 5월의 기온에 대한 전망은 어떤가요?\n",
      "\n",
      "정답: 2022년 5월의 기온 전망은 평년보다 낮을 확률이 20%, 비슷할 확률이 40%, 높을 확률이 40%입니다.\n",
      "\n",
      "예측: 2022년 5월의 기온은 평년보다 높을 확률이 50%이며, 대체로 평균보다 높겠음.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[7] 질문: 2023년 12월의 최대부하 전망은 얼마인가요?\n",
      "\n",
      "정답: 2023년 12월의 최대부하 전망은 72,700㎿입니다.\n",
      "\n",
      "예측: 2023년 12월의 최대부하는 94,300㎿로 예상됩니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[8] 질문: 8월의 기온 전망은 어떻게 되나요?\n",
      "\n",
      "정답: 8월의 기온 전망은 대체로 평년과 비슷하거나 높을 것으로 예상됩니다.\n",
      "\n",
      "예측: 8월 기온은 평년보다 높을 확률이 50%입니다.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[9] 질문: How does the electricity demand trend from 2015 to 2019 look?\n",
      "\n",
      "정답: Electricity demand generally increased from 2015 to 2018, but showed a slight decrease in 2019.\n",
      "\n",
      "예측: The electricity demand shows fluctuations with periods of growth and decline.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[10] 질문: 2018년 11월의 최대부하와 비교했을 때 2019년 11월은 몇 % 증가했나요?\n",
      "\n",
      "정답: 2019년 11월의 최대부하는 2018년 대비 7.8% 증가했습니다.\n",
      "\n",
      "예측: 2019년 11월의 최대부하는 76,800 MW로, 2018년 11월 대비 3.4% 증가했습니다.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test set 샘플로 성능 평가\n",
    "test_samples = dataset['test'].select(range(min(10, len(dataset['test']))))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test Set 샘플 평가\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    question = sample['messages'][0]['content']\n",
    "    expected = sample['messages'][1]['content']\n",
    "    predicted = test_model(question, max_new_tokens=200)\n",
    "    \n",
    "    print(f\"\\n[{i}] 질문: {question}\")\n",
    "    print(f\"\\n정답: {expected}\")\n",
    "    print(f\"\\n예측: {predicted}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 결론\n",
    "\n",
    "### SFT 접근법의 장점:\n",
    "- ✅ Instruction-following 능력 유지\n",
    "- ✅ 전력수요 도메인 지식 학습\n",
    "- ✅ 질문-답변 형태로 즉시 사용 가능\n",
    "\n",
    "### 다음 단계:\n",
    "1. **더 많은 데이터**: GPT API로 전체 125개 파일 처리\n",
    "2. **하이퍼파라미터 튜닝**: learning rate, epochs 조정\n",
    "3. **평가 메트릭**: BLEU, ROUGE 등으로 정량 평가\n",
    "4. **배포**: FastAPI 등으로 서빙\n",
    "\n",
    "### 비용 고려:\n",
    "- GPT-4o-mini: 약 $0.15/1M input tokens\n",
    "- 125개 파일 × 평균 5000 tokens = 625K tokens\n",
    "- 예상 비용: ~$0.10 (매우 저렴)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-llama",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
